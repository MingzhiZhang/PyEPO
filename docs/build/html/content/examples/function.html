

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Auto Grad Functions &mdash; PyTorch-based End-to-End Predict-then-Optimize Tool v0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=92fd9be5" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=fd3f3429" />

  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=34cd777e"></script>
      <script src="../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Solution Pool" href="pool.html" />
    <link rel="prev" title="Two-stage Method" href="twostage.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            PyTorch-based End-to-End Predict-then-Optimize Tool
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../tutorial.html">Tutorial</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="model.html">Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="data.html">Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="twostage.html">Two-stage Method</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Auto Grad Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#smart-predict-then-optimize-loss-spo">Smart Predict-then-Optimize+ Loss (SPO+)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#differentiable-black-box-optimizer-dbb">Differentiable Black-box Optimizer (DBB)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#negative-identity-backpropagation-nid">Negative Identity Backpropagation (NID)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#differentiable-perturbed-optimizer-dpo">Differentiable Perturbed Optimizer (DPO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#perturbed-fenchel-young-loss-pyfl">Perturbed Fenchel-Young Loss (PYFL)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#implicit-maximum-likelihood-estimator-i-mle">Implicit Maximum Likelihood Estimator (I-MLE)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-implicit-maximum-likelihood-estimator-ai-mle">Adaptive Implicit Maximum Likelihood Estimator (AI-MLE)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#noise-contrastive-estimation-nce">Noise Contrastive Estimation (NCE)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#contrastive-maximum-a-posterior-estimation-cmap">Contrastive Maximum A Posterior Estimation (CMAP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#learning-to-rank-ltr">Learning to Rank (LTR)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#perturbation-gradient-loss-pg">Perturbation Gradient Loss (PG)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#parallel-computation">Parallel Computation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pool.html">Solution Pool</a></li>
<li class="toctree-l2"><a class="reference internal" href="training.html">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="evaluation.html">Evaluation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ref.html">Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../autoapi/index.html">API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">PyTorch-based End-to-End Predict-then-Optimize Tool</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../tutorial.html">Tutorial</a></li>
      <li class="breadcrumb-item active">Auto Grad Functions</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/content/examples/function.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="auto-grad-functions">
<h1>Auto Grad Functions<a class="headerlink" href="#auto-grad-functions" title="Link to this heading"></a></h1>
<section id="smart-predict-then-optimize-loss-spo">
<h2>Smart Predict-then-Optimize+ Loss (SPO+)<a class="headerlink" href="#smart-predict-then-optimize-loss-spo" title="Link to this heading"></a></h2>
<p>SPO+ Loss function <a class="footnote-reference brackets" href="#f1" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> is a surrogate loss function of SPO Loss (Regret), which measures the decision error of optimization problem. For SPO/SPO+ Loss, the objective function is linear and constraints are known and fixed, but the cost vector need to be predicted from contextual data. The SPO+ Loss is convex with non-zero subgradient.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyepo.func.</span></span><span class="sig-name descname"><span class="pre">SPOPlus</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solve_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>An autograd module for SPO+ Loss, as a surrogate loss function of SPO
(regret) Loss, which measures the decision error of the optimization problem.</p>
<p>For SPO/SPO+ Loss, the objective function is linear and constraints are
known and fixed, but the cost vector needs to be predicted from contextual
data.</p>
<p>The SPO+ Loss is convex with subgradient. Thus, it allows us to design an
algorithm based on stochastic gradient descent.</p>
<p>Reference: &lt;<a class="reference external" href="https://doi.org/10.1287/mnsc.2020.3922">https://doi.org/10.1287/mnsc.2020.3922</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optmodel</strong> (<a class="reference internal" href="../../autoapi/pyepo/model/opt/index.html#pyepo.model.opt.optModel" title="pyepo.model.opt.optModel"><em>optModel</em></a>) – an PyEPO optimization model</p></li>
<li><p><strong>processes</strong> (<em>int</em>) – number of processors, 1 for single-core, 0 for all of cores</p></li>
<li><p><strong>solve_ratio</strong> (<em>float</em>) – the ratio of new solutions computed during training</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – the reduction to apply to the output</p></li>
<li><p><strong>dataset</strong> (<em>None/optDataset</em>) – the training data</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred_cost</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_cost</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_sol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_obj</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass</p>
</dd></dl>

</dd></dl>

<p><code class="docutils literal notranslate"><span class="pre">pyepo.func.SPOPlus</span></code> supports to solve optimization problems in parallel, parameter <code class="docutils literal notranslate"><span class="pre">processes</span></code> is the number of processors, <strong>0 for using all available cores</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyepo</span>

<span class="n">spo</span> <span class="o">=</span> <span class="n">pyepo</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">SPOPlus</span><span class="p">(</span><span class="n">optmodel</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="differentiable-black-box-optimizer-dbb">
<h2>Differentiable Black-box Optimizer (DBB)<a class="headerlink" href="#differentiable-black-box-optimizer-dbb" title="Link to this heading"></a></h2>
<p>Diffenretiable black-box (DBB) optimizer function <a class="footnote-reference brackets" href="#f2" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> estimates gradients from interpolation, replacing the zero gradients. For differentiable block-box, the objective function is linear and constraints are known and fixed, but the cost vector need to be predicted from contextual data.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyepo.func.</span></span><span class="sig-name descname"><span class="pre">blackboxOpt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solve_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>An autograd module for differentiable black-box optimizer, which yield
an optimal solution and derive a gradient.</p>
<p>For differentiable block-box, the objective function is linear and
constraints are known and fixed, but the cost vector needs to be predicted
from contextual data.</p>
<p>The block-box approximates the gradient of the optimizer by interpolating
the loss function. Thus, it allows us to design an algorithm based on
stochastic gradient descent.</p>
<p>Reference: &lt;<a class="reference external" href="https://arxiv.org/abs/1912.02175">https://arxiv.org/abs/1912.02175</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optmodel</strong> (<a class="reference internal" href="../../autoapi/pyepo/model/opt/index.html#pyepo.model.opt.optModel" title="pyepo.model.opt.optModel"><em>optModel</em></a>) – an PyEPO optimization model</p></li>
<li><p><strong>lambd</strong> (<em>float</em>) – a hyperparameter for differentiable block-box to control interpolation degree</p></li>
<li><p><strong>processes</strong> (<em>int</em>) – number of processors, 1 for single-core, 0 for all of cores</p></li>
<li><p><strong>solve_ratio</strong> (<em>float</em>) – the ratio of new solutions computed during training</p></li>
<li><p><strong>dataset</strong> (<em>None/optDataset</em>) – the training data</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred_cost</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass</p>
</dd></dl>

</dd></dl>

<p><code class="docutils literal notranslate"><span class="pre">pyepo.func.blackboxOpt</span></code> supports to solve optimization problems in parallel, parameter <code class="docutils literal notranslate"><span class="pre">processes</span></code> is the number of processors, 0 for using all available cores. <code class="docutils literal notranslate"><span class="pre">lambd</span></code> is a hyperparameter for function smoothing. The range of <code class="docutils literal notranslate"><span class="pre">lambd</span></code> should be <strong>10</strong> to <strong>20</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyepo</span>

<span class="n">dbb</span> <span class="o">=</span> <span class="n">pyepo</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">blackboxOpt</span><span class="p">(</span><span class="n">optmodel</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="negative-identity-backpropagation-nid">
<h2>Negative Identity Backpropagation (NID)<a class="headerlink" href="#negative-identity-backpropagation-nid" title="Link to this heading"></a></h2>
<p>Negative Identity Backpropagation (NID) <a class="footnote-reference brackets" href="#f6" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a> treats the solver as a negative identity mapping during the backward pass, which is equivalent to DBB with certain hyperparameter. It is hyperparameter-free and does not require any additional computationally expensive call to the solver on the backward pass.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyepo.func.</span></span><span class="sig-name descname"><span class="pre">negativeIdentity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solve_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>An autograd module for the differentiable optimizer, which yields optimal a
solution and use negative identity as a gradient on the backward pass.</p>
<p>For negative identity backpropagation, the objective function is linear and
constraints are known and fixed, but the cost vector needs to be predicted
from contextual data.</p>
<p>If the interpolation hyperparameter λ aligns with an appropriate step size,
then the identity update is equivalent to DBB. However, the identity update
does not require an additional call to the solver during the backward pass
and tuning an additional hyperparameter λ.</p>
<p>Reference: &lt;<a class="reference external" href="https://arxiv.org/abs/2205.15213">https://arxiv.org/abs/2205.15213</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optmodel</strong> (<a class="reference internal" href="../../autoapi/pyepo/model/opt/index.html#pyepo.model.opt.optModel" title="pyepo.model.opt.optModel"><em>optModel</em></a>) – an PyEPO optimization model</p></li>
<li><p><strong>processes</strong> (<em>int</em>) – number of processors, 1 for single-core, 0 for all of cores</p></li>
<li><p><strong>solve_ratio</strong> (<em>float</em>) – the ratio of new solutions computed during training</p></li>
<li><p><strong>dataset</strong> (<em>None/optDataset</em>) – the training data</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred_cost</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass</p>
</dd></dl>

</dd></dl>

<p><code class="docutils literal notranslate"><span class="pre">pyepo.func.negativeIdentity</span></code> supports to solve optimization problems in parallel, parameter <code class="docutils literal notranslate"><span class="pre">processes</span></code> is the number of processors, 0 for using all available cores.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyepo</span>

<span class="n">nid</span> <span class="o">=</span> <span class="n">pyepo</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">negativeIdentity</span><span class="p">(</span><span class="n">optmodel</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="differentiable-perturbed-optimizer-dpo">
<h2>Differentiable Perturbed Optimizer (DPO)<a class="headerlink" href="#differentiable-perturbed-optimizer-dpo" title="Link to this heading"></a></h2>
<p>Differentiable perturbed Optimizer (DPO) <a class="footnote-reference brackets" href="#f3" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> uses Monte-Carlo samples to estimate solutions, in which randomly perturbed costs are sampled to optimize. For the perturbed optimizer, the cost vector needs to be predicted from contextual data and are perturbed with Gaussian noise. The perturbed optimizer is differentiable in its inputs with non-zero Jacobian.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyepo.func.</span></span><span class="sig-name descname"><span class="pre">perturbedOpt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">135</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solve_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>An autograd module for Fenchel-Young loss using perturbation techniques. The
use of the loss improves the algorithmic by the specific expression of the
gradients of the loss.</p>
<p>For the perturbed optimizer, the cost vector needs to be predicted from
contextual data and are perturbed with Gaussian noise.</p>
<p>Thus, it allows us to design an algorithm based on stochastic gradient
descent.</p>
<p>Reference: &lt;<a class="reference external" href="https://papers.nips.cc/paper/2020/hash/6bb56208f672af0dd65451f869fedfd9-Abstract.html">https://papers.nips.cc/paper/2020/hash/6bb56208f672af0dd65451f869fedfd9-Abstract.html</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optmodel</strong> (<a class="reference internal" href="../../autoapi/pyepo/model/opt/index.html#pyepo.model.opt.optModel" title="pyepo.model.opt.optModel"><em>optModel</em></a>) – an PyEPO optimization model</p></li>
<li><p><strong>n_samples</strong> (<em>int</em>) – number of Monte-Carlo samples</p></li>
<li><p><strong>sigma</strong> (<em>float</em>) – the amplitude of the perturbation</p></li>
<li><p><strong>processes</strong> (<em>int</em>) – number of processors, 1 for single-core, 0 for all of cores</p></li>
<li><p><strong>seed</strong> (<em>int</em>) – random state seed</p></li>
<li><p><strong>solve_ratio</strong> (<em>float</em>) – the ratio of new solutions computed during training</p></li>
<li><p><strong>dataset</strong> (<em>None/optDataset</em>) – the training data</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred_cost</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass</p>
</dd></dl>

</dd></dl>

<p><code class="docutils literal notranslate"><span class="pre">pyepo.func.perturbedOpt</span></code> supports to solve optimization problems in parallel, parameter <code class="docutils literal notranslate"><span class="pre">processes</span></code> is the number of processors, 0 for using all available cores. <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> is the number of Monte-Carlo samples to estimate solutions, and <code class="docutils literal notranslate"><span class="pre">sigma</span></code> is the variance of Gaussian noise perturbation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyepo</span>

<span class="n">dpo</span> <span class="o">=</span> <span class="n">pyepo</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">perturbedOpt</span><span class="p">(</span><span class="n">optmodel</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="perturbed-fenchel-young-loss-pyfl">
<h2>Perturbed Fenchel-Young Loss (PYFL)<a class="headerlink" href="#perturbed-fenchel-young-loss-pyfl" title="Link to this heading"></a></h2>
<p>Perturbed Fenchel-Young loss (PYFL) function <a class="footnote-reference brackets" href="#f3" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> uses perturbation techniques with Monte-Carlo samples. The use of the loss improves the algorithmic by the specific expression of the gradients of the loss. For the perturbed optimizer, the cost vector need to be predicted from contextual data and are perturbed with Gaussian noise. The Fenchel-Young loss allows to directly optimize a loss between the features and solutions with less computation.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyepo.func.</span></span><span class="sig-name descname"><span class="pre">perturbedFenchelYoung</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">135</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solve_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>An autograd module for Fenchel-Young loss using perturbation techniques. The
use of the loss improves the algorithmic by the specific expression of the
gradients of the loss.</p>
<p>For the perturbed optimizer, the cost vector need to be predicted from
contextual data and are perturbed with Gaussian noise.</p>
<p>The Fenchel-Young loss allows to directly optimize a loss between the features
and solutions with less computation. Thus, allows us to design an algorithm
based on stochastic gradient descent.</p>
<p>Reference: &lt;<a class="reference external" href="https://papers.nips.cc/paper/2020/hash/6bb56208f672af0dd65451f869fedfd9-Abstract.html">https://papers.nips.cc/paper/2020/hash/6bb56208f672af0dd65451f869fedfd9-Abstract.html</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optmodel</strong> (<a class="reference internal" href="../../autoapi/pyepo/model/opt/index.html#pyepo.model.opt.optModel" title="pyepo.model.opt.optModel"><em>optModel</em></a>) – an PyEPO optimization model</p></li>
<li><p><strong>n_samples</strong> (<em>int</em>) – number of Monte-Carlo samples</p></li>
<li><p><strong>sigma</strong> (<em>float</em>) – the amplitude of the perturbation</p></li>
<li><p><strong>processes</strong> (<em>int</em>) – number of processors, 1 for single-core, 0 for all of cores</p></li>
<li><p><strong>seed</strong> (<em>int</em>) – random state seed</p></li>
<li><p><strong>solve_ratio</strong> (<em>float</em>) – the ratio of new solutions computed during training</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – the reduction to apply to the output</p></li>
<li><p><strong>dataset</strong> (<em>None/optDataset</em>) – the training data</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred_cost</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_sol</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass</p>
</dd></dl>

</dd></dl>

<p><code class="docutils literal notranslate"><span class="pre">pyepo.func.perturbedFenchelYoung</span></code> supports to solve optimization problems in parallel, parameter <code class="docutils literal notranslate"><span class="pre">processes</span></code> is the number of processors, 0 for using all available cores. <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> is the number of Monte-Carlo samples to estimate solutions, and <code class="docutils literal notranslate"><span class="pre">sigma</span></code> is the variance of Gaussian noise perturbation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyepo</span>

<span class="n">pfy</span> <span class="o">=</span> <span class="n">pyepo</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">perturbedFenchelYoung</span><span class="p">(</span><span class="n">optmodel</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="implicit-maximum-likelihood-estimator-i-mle">
<h2>Implicit Maximum Likelihood Estimator (I-MLE)<a class="headerlink" href="#implicit-maximum-likelihood-estimator-i-mle" title="Link to this heading"></a></h2>
<p>Implicit Maximum Likelihood Estimator (I-MLE) <a class="footnote-reference brackets" href="#f7" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> use the perturb-and-MAP framework. They sample noise from a Sum-of-Gamma distribution and interpolate the loss function to approximate finite difference.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyepo.func.</span></span><span class="sig-name descname"><span class="pre">implicitMLE</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma=1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambd=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distribution=&lt;pyepo.func.utlis.sumGammaDistribution</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">two_sides=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processes=1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solve_ratio=1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset=None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>An autograd module for Implicit Maximum Likelihood Estimator, which yield
an optimal solution in a constrained exponential family distribution via
Perturb-and-MAP.</p>
<p>For I-MLE, it works as black-box combinatorial solvers, in which constraints
are known and fixed, but the cost vector need to be predicted from
contextual data.</p>
<p>The I-MLE approximate gradient of optimizer smoothly. Thus, allows us to
design an algorithm based on stochastic gradient descent.</p>
<p>Reference: &lt;<a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/7a430339c10c642c4b2251756fd1b484-Abstract.html">https://proceedings.neurips.cc/paper_files/paper/2021/hash/7a430339c10c642c4b2251756fd1b484-Abstract.html</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optmodel</strong> (<a class="reference internal" href="../../autoapi/pyepo/model/opt/index.html#pyepo.model.opt.optModel" title="pyepo.model.opt.optModel"><em>optModel</em></a>) – an PyEPO optimization model</p></li>
<li><p><strong>n_samples</strong> (<em>int</em>) – number of Monte-Carlo samples</p></li>
<li><p><strong>sigma</strong> (<em>float</em>) – noise temperature for the input distribution</p></li>
<li><p><strong>lambd</strong> (<em>float</em>) – a hyperparameter for differentiable block-box to control interpolation degree</p></li>
<li><p><strong>distribution</strong> (<em>distribution</em>) – noise distribution</p></li>
<li><p><strong>two_sides</strong> (<em>bool</em>) – approximate gradient by two-sided perturbation or not</p></li>
<li><p><strong>processes</strong> (<em>int</em>) – number of processors, 1 for single-core, 0 for all of cores</p></li>
<li><p><strong>solve_ratio</strong> (<em>float</em>) – the ratio of new solutions computed during training</p></li>
<li><p><strong>dataset</strong> (<em>None/optDataset</em>) – the training data</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred_cost</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass</p>
</dd></dl>

</dd></dl>

<p><code class="docutils literal notranslate"><span class="pre">pyepo.func.implicitMLE</span></code> supports to solve optimization problems in parallel, parameter <code class="docutils literal notranslate"><span class="pre">processes</span></code> is the number of processors, 0 for using all available cores.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyepo</span>

<span class="n">imle</span> <span class="o">=</span> <span class="n">pyepo</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">implicitMLE</span><span class="p">(</span><span class="n">optmodel</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="adaptive-implicit-maximum-likelihood-estimator-ai-mle">
<h2>Adaptive Implicit Maximum Likelihood Estimator (AI-MLE)<a class="headerlink" href="#adaptive-implicit-maximum-likelihood-estimator-ai-mle" title="Link to this heading"></a></h2>
<p>Adaptive Implicit Maximum Likelihood Estimator (AI-MLE) <a class="footnote-reference brackets" href="#f8" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> use the adaptive interpolation step and the perturb-and-MAP framework. They sample noise from a Sum-of-Gamma distribution and interpolate the loss function to approximate finite difference.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyepo.func.</span></span><span class="sig-name descname"><span class="pre">adaptiveImplicitMLE</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma=1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distribution=&lt;pyepo.func.utlis.sumGammaDistribution</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">two_sides=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processes=1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solve_ratio=1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset=None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>An autograd module for Adaptive Implicit Maximum Likelihood Estimator, which
adaptively choose hyperparameter λ and yield an optimal solution in a
constrained exponential family distribution via Perturb-and-MAP.</p>
<p>For AI-MLE, it works as black-box combinatorial solvers, in which constraints
are known and fixed, but the cost vector need to be predicted from
contextual data.</p>
<p>The AI-MLE approximate gradient of optimizer smoothly. Thus, allows us to
design an algorithm based on stochastic gradient descent.</p>
<p>Reference: &lt;<a class="reference external" href="https://ojs.aaai.org/index.php/AAAI/article/view/26103">https://ojs.aaai.org/index.php/AAAI/article/view/26103</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optmodel</strong> (<a class="reference internal" href="../../autoapi/pyepo/model/opt/index.html#pyepo.model.opt.optModel" title="pyepo.model.opt.optModel"><em>optModel</em></a>) – an PyEPO optimization model</p></li>
<li><p><strong>n_samples</strong> (<em>int</em>) – number of Monte-Carlo samples</p></li>
<li><p><strong>sigma</strong> (<em>float</em>) – noise temperature for the input distribution</p></li>
<li><p><strong>distribution</strong> (<em>distribution</em>) – noise distribution</p></li>
<li><p><strong>two_sides</strong> (<em>bool</em>) – approximate gradient by two-sided perturbation or not</p></li>
<li><p><strong>processes</strong> (<em>int</em>) – number of processors, 1 for single-core, 0 for all of cores</p></li>
<li><p><strong>solve_ratio</strong> (<em>float</em>) – the ratio of new solutions computed during training</p></li>
<li><p><strong>dataset</strong> (<em>None/optDataset</em>) – the training data</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred_cost</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass</p>
</dd></dl>

</dd></dl>

<p><code class="docutils literal notranslate"><span class="pre">pyepo.func.adaptiveImplicitMLE</span></code> supports to solve optimization problems in parallel, parameter <code class="docutils literal notranslate"><span class="pre">processes</span></code> is the number of processors, 0 for using all available cores.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyepo</span>

<span class="n">aimle</span> <span class="o">=</span> <span class="n">pyepo</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">adaptiveImplicitMLE</span><span class="p">(</span><span class="n">optmodel</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="noise-contrastive-estimation-nce">
<h2>Noise Contrastive Estimation (NCE)<a class="headerlink" href="#noise-contrastive-estimation-nce" title="Link to this heading"></a></h2>
<p>Noise Contrastive Estimation (NCE) <a class="footnote-reference brackets" href="#f4" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> serve as surrogate loss function based on negative examples. The key idea is to work with a small set of non-optimal solutions as negative samples. Thus, we can maximizes the difference  of the probability between optimal solution and others.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyepo.func.</span></span><span class="sig-name descname"><span class="pre">NCE</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solve_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>An autograd module for noise contrastive estimation as surrogate loss
functions, based on viewing suboptimal solutions as negative examples.</p>
<p>For the NCE, the cost vector needs to be predicted from contextual data and
maximizes the separation of the probability of the optimal solution.</p>
<p>Thus allows us to design an algorithm based on stochastic gradient descent.</p>
<p>Reference: &lt;<a class="reference external" href="https://www.ijcai.org/proceedings/2021/390">https://www.ijcai.org/proceedings/2021/390</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optmodel</strong> (<a class="reference internal" href="../../autoapi/pyepo/model/opt/index.html#pyepo.model.opt.optModel" title="pyepo.model.opt.optModel"><em>optModel</em></a>) – an PyEPO optimization model</p></li>
<li><p><strong>processes</strong> (<em>int</em>) – number of processors, 1 for single-core, 0 for all of cores</p></li>
<li><p><strong>solve_ratio</strong> (<em>float</em>) – the ratio of new solutions computed during training</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – the reduction to apply to the output</p></li>
<li><p><strong>dataset</strong> (<em>None/optDataset</em>) – the training data, usually this is simply the training set</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred_cost</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_sol</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass</p>
</dd></dl>

</dd></dl>

<p><code class="docutils literal notranslate"><span class="pre">pyepo.func.NCE</span></code> supports to solve optimization problems in parallel, parameter <code class="docutils literal notranslate"><span class="pre">processes</span></code> is the number of processors, <strong>0 for using all available cores</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyepo</span>

<span class="n">nce</span> <span class="o">=</span> <span class="n">pyepo</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">NCE</span><span class="p">(</span><span class="n">optmodel</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">solve_ratio</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">dataset_train</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="contrastive-maximum-a-posterior-estimation-cmap">
<h2>Contrastive Maximum A Posterior Estimation (CMAP)<a class="headerlink" href="#contrastive-maximum-a-posterior-estimation-cmap" title="Link to this heading"></a></h2>
<p>Contrastive Maximum A Posteriori (CMAP) Loss function <a class="footnote-reference brackets" href="#f4" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> is a special case of NCE where only samples the best one. It is simple but efficient.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyepo.func.</span></span><span class="sig-name descname"><span class="pre">contrastiveMAP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solve_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>An autograd module for Maximum A Posterior contrastive estimation as
surrogate loss functions, which is an efficient self-contrastive algorithm.</p>
<p>For the MAP, the cost vector needs to be predicted from contextual data and
maximizes the separation of the probability of the optimal solution.</p>
<p>Thus, it allows us to design an algorithm based on stochastic gradient descent.</p>
<p>Reference: &lt;<a class="reference external" href="https://www.ijcai.org/proceedings/2021/390">https://www.ijcai.org/proceedings/2021/390</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optmodel</strong> (<a class="reference internal" href="../../autoapi/pyepo/model/opt/index.html#pyepo.model.opt.optModel" title="pyepo.model.opt.optModel"><em>optModel</em></a>) – an PyEPO optimization model</p></li>
<li><p><strong>processes</strong> (<em>int</em>) – number of processors, 1 for single-core, 0 for all of cores</p></li>
<li><p><strong>solve_ratio</strong> (<em>float</em>) – the ratio of new solutions computed during training</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – the reduction to apply to the output</p></li>
<li><p><strong>dataset</strong> (<em>None/optDataset</em>) – the training data, usually this is simply the training set</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred_cost</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_sol</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass</p>
</dd></dl>

</dd></dl>

<p><code class="docutils literal notranslate"><span class="pre">pyepo.func.contrastiveMAP</span></code> supports to solve optimization problems in parallel, parameter <code class="docutils literal notranslate"><span class="pre">processes</span></code> is the number of processors, <strong>0 for using all available cores</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyepo</span>

<span class="n">cmap</span> <span class="o">=</span> <span class="n">pyepo</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">contrastiveMAP</span><span class="p">(</span><span class="n">optmodel</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">solve_ratio</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">dataset_train</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="learning-to-rank-ltr">
<h2>Learning to Rank (LTR)<a class="headerlink" href="#learning-to-rank-ltr" title="Link to this heading"></a></h2>
<p>LTR Loss function <a class="footnote-reference brackets" href="#f5" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> is to learn an objective function that ranks a pool of feasible solutions correctly. LTR methods assign scores to the disparate solutions in pool, then establish surrogate loss functions predicated on these scores with the intention of ranking the optimal solution best.</p>
<p>Pointwise loss calculates the ranking scores of the items.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyepo.func.</span></span><span class="sig-name descname"><span class="pre">pointwiseLTR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solve_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>An autograd module for pointwise learning to rank, where the goal is to
learn an objective function that ranks a pool of feasible solutions
correctly.</p>
<p>For the pointwise LTR, the cost vector needs to be predicted from contextual
data, and calculates the ranking scores of the items.</p>
<p>Thus, it allows us to design an algorithm based on stochastic gradient
descent.</p>
<p>Reference: &lt;<a class="reference external" href="https://proceedings.mlr.press/v162/mandi22a.html">https://proceedings.mlr.press/v162/mandi22a.html</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optmodel</strong> (<a class="reference internal" href="../../autoapi/pyepo/model/opt/index.html#pyepo.model.opt.optModel" title="pyepo.model.opt.optModel"><em>optModel</em></a>) – an PyEPO optimization model</p></li>
<li><p><strong>processes</strong> (<em>int</em>) – number of processors, 1 for single-core, 0 for all of cores</p></li>
<li><p><strong>solve_ratio</strong> (<em>float</em>) – the ratio of new solutions computed during training</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – the reduction to apply to the output</p></li>
<li><p><strong>dataset</strong> (<a class="reference internal" href="../../autoapi/pyepo/data/dataset/index.html#pyepo.data.dataset.optDataset" title="pyepo.data.dataset.optDataset"><em>optDataset</em></a>) – the training data</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred_cost</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_cost</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass</p>
</dd></dl>

</dd></dl>

<p>Pairwise loss learns the relative ordering of pairs of items.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyepo.func.</span></span><span class="sig-name descname"><span class="pre">pairwiseLTR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solve_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>An autograd module for pairwise learning to rank, where the goal is to learn
an objective function that ranks a pool of feasible solutions correctly.</p>
<p>For the pairwise LTR, the cost vector needs to be predicted from the
contextual data and the loss learns the relative ordering of pairs of items.</p>
<p>Thus, it allows us to design an algorithm based on stochastic gradient
descent.</p>
<p>Reference: &lt;<a class="reference external" href="https://proceedings.mlr.press/v162/mandi22a.html">https://proceedings.mlr.press/v162/mandi22a.html</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optmodel</strong> (<a class="reference internal" href="../../autoapi/pyepo/model/opt/index.html#pyepo.model.opt.optModel" title="pyepo.model.opt.optModel"><em>optModel</em></a>) – an PyEPO optimization model</p></li>
<li><p><strong>processes</strong> (<em>int</em>) – number of processors, 1 for single-core, 0 for all of cores</p></li>
<li><p><strong>solve_ratio</strong> (<em>float</em>) – the ratio of new solutions computed during training</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – the reduction to apply to the output</p></li>
<li><p><strong>dataset</strong> (<a class="reference internal" href="../../autoapi/pyepo/data/dataset/index.html#pyepo.data.dataset.optDataset" title="pyepo.data.dataset.optDataset"><em>optDataset</em></a>) – the training data</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred_cost</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_cost</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass</p>
</dd></dl>

</dd></dl>

<p>Listwise loss measures the scores of the whole ranked lists.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyepo.func.</span></span><span class="sig-name descname"><span class="pre">listwiseLTR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solve_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>An autograd module for listwise learning to rank, where the goal is to learn
an objective function that ranks a pool of feasible solutions correctly.</p>
<p>For the listwise LTR, the cost vector needs to be predicted from the
contextual data and the loss measures the scores of the whole ranked lists.</p>
<p>Thus, it allows us to design an algorithm based on stochastic gradient
descent.</p>
<p>Reference: &lt;<a class="reference external" href="https://proceedings.mlr.press/v162/mandi22a.html">https://proceedings.mlr.press/v162/mandi22a.html</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optmodel</strong> (<a class="reference internal" href="../../autoapi/pyepo/model/opt/index.html#pyepo.model.opt.optModel" title="pyepo.model.opt.optModel"><em>optModel</em></a>) – an PyEPO optimization model</p></li>
<li><p><strong>processes</strong> (<em>int</em>) – number of processors, 1 for single-core, 0 for all of cores</p></li>
<li><p><strong>solve_ratio</strong> (<em>float</em>) – the ratio of new solutions computed during training</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – the reduction to apply to the output</p></li>
<li><p><strong>dataset</strong> (<a class="reference internal" href="../../autoapi/pyepo/data/dataset/index.html#pyepo.data.dataset.optDataset" title="pyepo.data.dataset.optDataset"><em>optDataset</em></a>) – the training data, usually this is simply the training set</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred_cost</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_cost</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass</p>
</dd></dl>

</dd></dl>

<p><code class="docutils literal notranslate"><span class="pre">pyepo.func.pointwiseLTR</span></code>, <code class="docutils literal notranslate"><span class="pre">pyepo.func.pairwiseLTR</span></code>, and <code class="docutils literal notranslate"><span class="pre">pyepo.func.listwiseLTR</span></code> supports to solve optimization problems in parallel, parameter <code class="docutils literal notranslate"><span class="pre">processes</span></code> is the number of processors, <strong>0 for using all available cores</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyepo</span>

<span class="c1"># pointwise</span>
<span class="n">ltr</span> <span class="o">=</span> <span class="n">pyepo</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">pointwiseLTR</span><span class="p">(</span><span class="n">optmodel</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">solve_ratio</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">dataset_train</span><span class="p">)</span>
<span class="c1"># pairwise</span>
<span class="n">ltr</span> <span class="o">=</span> <span class="n">pyepo</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">pairwiseLTR</span><span class="p">(</span><span class="n">optmodel</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">solve_ratio</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">dataset_train</span><span class="p">)</span>
<span class="c1"># listwise</span>
<span class="n">ltr</span> <span class="o">=</span> <span class="n">pyepo</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">listwiseLTR</span><span class="p">(</span><span class="n">optmodel</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">solve_ratio</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">dataset_train</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="perturbation-gradient-loss-pg">
<h2>Perturbation Gradient Loss (PG)<a class="headerlink" href="#perturbation-gradient-loss-pg" title="Link to this heading"></a></h2>
<p>PG Loss function <a class="footnote-reference brackets" href="#f9" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>  a surrogate loss function of objective value, which measures the decision quality of the optimization problem. For PG Loss, the objective function is linear, and constraints are known and fixed, but the cost vector needs to be predicted from contextual data. According to Danskin’s Theorem, the PG Loss is derived from different zeroth order approximations and has the informative gradient. Thus, it allows us to design an algorithm based on stochastic gradient descent.</p>
<dl class="py class">
<dt class="sig sig-object py">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pyepo.func.</span></span><span class="sig-name descname"><span class="pre">perturbationGradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">two_sides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">processes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solve_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>An autograd module for PG Loss, as a surrogate loss function of objective
value, which measures the decision quality of the optimization problem.</p>
<p>For PG Loss, the objective function is linear, and constraints are
known and fixed, but the cost vector needs to be predicted from contextual
data.</p>
<p>According to Danskin’s Theorem, the PG Loss is derived from different zeroth
order approximations and has the informative gradient. Thus, it allows us to
design an algorithm based on stochastic gradient descent.</p>
<p>Reference: &lt;<a class="reference external" href="https://arxiv.org/abs/2402.03256">https://arxiv.org/abs/2402.03256</a>&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optmodel</strong> (<a class="reference internal" href="../../autoapi/pyepo/model/opt/index.html#pyepo.model.opt.optModel" title="pyepo.model.opt.optModel"><em>optModel</em></a>) – an PyEPO optimization model</p></li>
<li><p><strong>sigma</strong> (<em>float</em>) – the amplitude of the finite difference width used for loss approximation</p></li>
<li><p><strong>two_sides</strong> (<em>bool</em>) – approximate gradient by two-sided perturbation or not</p></li>
<li><p><strong>processes</strong> (<em>int</em>) – number of processors, 1 for single-core, 0 for all of cores</p></li>
<li><p><strong>solve_ratio</strong> (<em>float</em>) – the ratio of new solutions computed during training</p></li>
<li><p><strong>reduction</strong> (<em>str</em>) – the reduction to apply to the output</p></li>
<li><p><strong>dataset</strong> (<em>None/optDataset</em>) – the training data</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pred_cost</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_cost</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>Forward pass</p>
</dd></dl>

</dd></dl>

<p><code class="docutils literal notranslate"><span class="pre">pyepo.func.perturbationGradient</span></code> supports to solve optimization problems in parallel, parameter <code class="docutils literal notranslate"><span class="pre">processes</span></code> is the number of processors, <strong>0 for using all available cores</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyepo</span>

<span class="n">pg</span> <span class="o">=</span> <span class="n">pyepo</span><span class="o">.</span><span class="n">func</span><span class="o">.</span><span class="n">perturbationGradient</span><span class="p">(</span><span class="n">optmodel</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">two_sides</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">processes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="parallel-computation">
<h2>Parallel Computation<a class="headerlink" href="#parallel-computation" title="Link to this heading"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">PyEPO</span></code> supports parallel computation for solving optimization problems in training, where the parameter <code class="docutils literal notranslate"><span class="pre">processes</span></code> is the number of processors to be used.</p>
<a class="reference internal image-reference" href="../../_images/parallel-tsp.png"><img alt="../../_images/parallel-tsp.png" src="../../_images/parallel-tsp.png" style="width: 650px;" /></a>
<p>The figure shows that the increasing of processes reduces the runtime.</p>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="f1" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Elmachtoub, A. N., &amp; Grigas, P. (2021). Smart “predict, then optimize”. Management Science.</p>
</aside>
<aside class="footnote brackets" id="f2" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Vlastelica, M., Paulus, A., Musil, V., Martius, G., &amp; Rolínek, M. (2019). Differentiation of blackbox combinatorial solvers. arXiv preprint arXiv:1912.02175.</p>
</aside>
<aside class="footnote brackets" id="f3" role="note">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id4">1</a>,<a role="doc-backlink" href="#id5">2</a>)</span>
<p>Berthet, Q., Blondel, M., Teboul, O., Cuturi, M., Vert, J. P., &amp; Bach, F. (2020). Learning with differentiable perturbed optimizers. Advances in neural information processing systems, 33, 9508-9519.</p>
</aside>
<aside class="footnote brackets" id="f4" role="note">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id8">1</a>,<a role="doc-backlink" href="#id9">2</a>)</span>
<p>Mulamba, M., Mandi, J., Diligenti, M., Lombardi, M., Bucarey, V., &amp; Guns, T. (2021). Contrastive losses and solution caching for predict-and-optimize. Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence.</p>
</aside>
<aside class="footnote brackets" id="f5" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">5</a><span class="fn-bracket">]</span></span>
<p>Mandi, J., Bucarey, V., Mulamba, M., &amp; Guns, T. (2022). Decision-focused learning: through the lens of learning to rank. Proceedings of the 39th International Conference on Machine Learning.</p>
</aside>
<aside class="footnote brackets" id="f6" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">6</a><span class="fn-bracket">]</span></span>
<p>Sahoo, S. S., Paulus, A., Vlastelica, M., Musil, V., Kuleshov, V., &amp; Martius, G. (2022). Backpropagation through combinatorial algorithms: Identity with projection works. arXiv preprint arXiv:2205.15213.</p>
</aside>
<aside class="footnote brackets" id="f7" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">7</a><span class="fn-bracket">]</span></span>
<p>Niepert, M., Minervini, P., &amp; Franceschi, L. (2021). Implicit MLE: backpropagating through discrete exponential family distributions. Advances in Neural Information Processing Systems, 34, 14567-14579.</p>
</aside>
<aside class="footnote brackets" id="f8" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">8</a><span class="fn-bracket">]</span></span>
<p>Minervini, P., Franceschi, L., &amp; Niepert, M. (2023, June). Adaptive perturbation-based gradient estimation for discrete latent variable models. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 8, pp. 9200-9208).</p>
</aside>
<aside class="footnote brackets" id="f9" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">9</a><span class="fn-bracket">]</span></span>
<p>Gupta, V., &amp; Huang, M. (2024). Decision-Focused Learning with Directional Gradients. Training, 50(100), 150.</p>
</aside>
</aside>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="twostage.html" class="btn btn-neutral float-left" title="Two-stage Method" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="pool.html" class="btn btn-neutral float-right" title="Solution Pool" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Bo Tang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>