:mod:`pyepo.func`
=================

.. py:module:: pyepo.func

.. autoapi-nested-parse::

   Pytorch autograd function for SPO training



Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   blackbox/index.rst
   spoplus/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   pyepo.func.blackboxOpt
   pyepo.func.SPOPlus



.. py:class:: blackboxOpt(optmodel, lambd=10, processes=1, solve_ratio=1, dataset=None)

   Bases: :class:`torch.nn.Module`

   A autograd module for differentiable black-box optimizer, which yield
   optimal a solution and derive a gradient.

   For differentiable block-box, the objective function is linear and
   constraints are known and fixed, but the cost vector need to be predicted
   from contextual data.

   The block-box approximate gradient of optimizer smoothly. Thus, allows us to
   design an algorithm based on stochastic gradient descent.

   .. method:: forward(self, pred_cost)

      Forward pass



.. py:class:: SPOPlus(optmodel, processes=1, solve_ratio=1, dataset=None)

   Bases: :class:`torch.nn.Module`

   A autograd module for SPO+ Loss, as a surrogate loss function of SPO Loss,
   which measures the decision error of optimization problem.

   For SPO/SPO+ Loss, the objective function is linear and constraints are
   known and fixed, but the cost vector need to be predicted from contextual
   data.

   The SPO+ Loss is convex with subgradient. Thus, allows us to design an
   algorithm based on stochastic gradient descent.

   .. method:: forward(self, pred_cost, true_cost, true_sol, true_obj)

      Forward pass



