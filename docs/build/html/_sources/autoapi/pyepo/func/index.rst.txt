:mod:`pyepo.func`
=================

.. py:module:: pyepo.func

.. autoapi-nested-parse::

   Pytorch autograd function for SPO training



Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   blackbox/index.rst
   perturbed/index.rst
   spoplus/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   pyepo.func.blackboxOpt
   pyepo.func.SPOPlus
   pyepo.func.perturbedOpt
   pyepo.func.perturbedFenchelYoung



.. py:class:: blackboxOpt(optmodel, lambd=10, processes=1, solve_ratio=1, dataset=None)

   Bases: :class:`torch.nn.Module`

   A autograd module for differentiable black-box optimizer, which yield
   optimal a solution and derive a gradient.

   For differentiable block-box, the objective function is linear and
   constraints are known and fixed, but the cost vector need to be predicted
   from contextual data.

   The block-box approximate gradient of optimizer smoothly. Thus, allows us to
   design an algorithm based on stochastic gradient descent.

   .. method:: forward(self, pred_cost)

      Forward pass



.. py:class:: SPOPlus(optmodel, processes=1, solve_ratio=1, dataset=None)

   Bases: :class:`torch.nn.Module`

   A autograd module for SPO+ Loss, as a surrogate loss function of SPO Loss,
   which measures the decision error of optimization problem.

   For SPO/SPO+ Loss, the objective function is linear and constraints are
   known and fixed, but the cost vector need to be predicted from contextual
   data.

   The SPO+ Loss is convex with subgradient. Thus, allows us to design an
   algorithm based on stochastic gradient descent.

   .. method:: forward(self, pred_cost, true_cost, true_sol, true_obj)

      Forward pass



.. py:class:: perturbedOpt(optmodel, n_samples=10, epsilon=1.0, processes=1, solve_ratio=1, dataset=None)

   Bases: :class:`torch.nn.Module`

   A autograd module for perturbed optimizer, models of random optimizers with
   perturbed inputs.

   For the perturbed optimizer, the cost vector need to be predicted from
   contextual data and are perturbed with Gaussian noise.

   The perturbed optimizer differentiable in its inputs with non-zero Jacobian.
   Thus, allows us to design an algorithm based on stochastic gradient descent.

   .. method:: forward(self, pred_cost)

      Forward pass



.. py:class:: perturbedFenchelYoung(optmodel, n_samples=10, epsilon=1.0, processes=1, solve_ratio=1, dataset=None)

   Bases: :class:`torch.nn.Module`

   A autograd module for Fenchel-Young loss using perturbation techniques.The
   use of the loss improves the algorithmic by the specific expression of the
   gradients of the loss.

   For the perturbed optimizer, the cost vector need to be predicted from
   contextual data and are perturbed with Gaussian noise.

   The Fenchel-Young loss allows to directly optimize a loss between the features
   and solutions with less computation. Thus, allows us to design an algorithm
   based on stochastic gradient descent.

   .. method:: forward(self, pred_cost, true_sol)

      Forward pass



